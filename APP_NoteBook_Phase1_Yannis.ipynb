{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7d879",
   "metadata": {},
   "source": [
    "# Vidéo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet1_f-42e0d11a\\\\e0b2c246_0.0-138.011.mp4\"              # chemin vers ta vidéo POV\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet1_f-42e0d11a\\\\gaze.csv\"                      # chemin vers ton fichier CSV de données de regard\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet1_f-42e0d11a\\\\world_timestamps.csv\"  # chemin vers ton fichier CSV de timestamps mondiaux\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet1.mp4\"\n",
    "\n",
    "# Charger paramètres caméra, ça nous sera bien utile pour plus tard !\n",
    "with open(\"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet1_f-42e0d11a\\\\scene_camera.json\", \"r\") as f:\n",
    "    cam = json.load(f)\n",
    "\n",
    "K = np.array(cam[\"camera_matrix\"])\n",
    "dist = np.array(cam[\"distortion_coefficients\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58c2e8",
   "metadata": {},
   "source": [
    "# Vidéo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet2_f-835bf855\\\\b7bd6c34_0.0-271.583.mp4\"              # chemin vers ta vidéo POV\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet2_f-835bf855\\\\gaze.csv\"                      # chemin vers ton fichier CSV de données de regard\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet2_f-835bf855\\\\world_timestamps.csv\"  # chemin vers ton fichier CSV de timestamps mondiaux\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet2.mp4\"\n",
    "\n",
    "# Charger paramètres caméra, plus tard on le re-utilisera d'ailleurs\n",
    "with open(\"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet2_f-835bf855\\\\scene_camera.json\", \"r\") as f:\n",
    "    cam = json.load(f)\n",
    "\n",
    "K = np.array(cam[\"camera_matrix\"])\n",
    "dist = np.array(cam[\"distortion_coefficients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f84ede",
   "metadata": {},
   "source": [
    "# Vidéo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee248127",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet3_m-84ce1158\\\\422f10f2_0.0-247.734.mp4\"\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet3_m-84ce1158\\\\gaze.csv\"\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet3_m-84ce1158\\\\world_timestamps.csv\"\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet3.mp4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1620d",
   "metadata": {},
   "source": [
    "# Vidéo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fade60",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet4_m-fee537df\\\\2fb8301a_0.0-71.632.mp4\"\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet4_m-fee537df\\\\gaze.csv\"\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet4_m-fee537df\\\\world_timestamps.csv\"\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet4.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc6b0e",
   "metadata": {},
   "source": [
    "# Vidéo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a85df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet5_m-671cf44e\\\\585d8df7_0.0-229.268.mp4\"\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet5_m-671cf44e\\\\gaze.csv\"\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet5_m-671cf44e\\\\world_timestamps.csv\"\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet5.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd1991",
   "metadata": {},
   "source": [
    "# Vidéo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec63a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet6_m-0b355b51\\\\429d311a_0.0-267.743.mp4\"\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet6_m-0b355b51\\\\gaze.csv\"\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet6_m-0b355b51\\\\world_timestamps.csv\"\n",
    "OUTPUT_VIDEO_PATH = \"world_with_gaze_Sujet6.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281dfacb",
   "metadata": {},
   "source": [
    "### Analyse technique du fonctionnement du programme\n",
    "\n",
    "Le script s’organise en deux grandes étapes :  \n",
    "1) *Synchroniser le regard avec chaque frame de la vidéo*  \n",
    "2) *Générer une nouvelle vidéo avec le point de regard superposé*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Préparation et synchronisation temporelle\n",
    "\n",
    "* On charge les données de regard (`gaze.csv`) et les timestamps vidéo (`world_timestamps.csv`).  \n",
    "* On ne conserve que les colonnes nécessaires (timestamp + coordonnées du regard) et on trie tout chronologiquement.  \n",
    "* On calcule la *période moyenne entre deux frames vidéo* pour estimer le rythme d’acquisition.  \n",
    "* À partir de cette période, on définit une *tolérance temporelle* :  \n",
    "  * elle fixe jusqu’à quel écart temporel un point de regard est encore acceptable pour une frame.  \n",
    "* `merge_asof` orchestre alors la *fusion temporelle* :  \n",
    "  * pour chaque frame, il récupère **l’échantillon de regard le plus proche dans le temps**,  \n",
    "  * mais uniquement s’il se situe dans la tolérance définie.\n",
    "\n",
    "→ Résultat : une table alignée où chaque frame vidéo possède soit une coordonnée de regard fiable, soit `NaN`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Lecture vidéo et overlay du point de regard\n",
    "\n",
    "* On ouvre la vidéo en lecture et on récupère ses propriétés (fps, largeur, hauteur).  \n",
    "* On initialise un `VideoWriter` pour produire la vidéo annotée.  \n",
    "* Dans une boucle frame par frame :\n",
    "  * on lit l’image,\n",
    "  * on extrait la coordonnée de regard associée,\n",
    "  * si elle existe (pas de `NaN`), on :\n",
    "    * “clamp” la coordonnée dans l’image,\n",
    "    * **dessine un cercle rouge** au bon emplacement.\n",
    "  * On écrit ensuite la frame annotée dans la vidéo de sortie.\n",
    "\n",
    "* À la fin, on appelle `cap.release()` et `out.release()` pour fermer correctement les flux vidéo.\n",
    "\n",
    "---\n",
    "\n",
    "### Intention globale\n",
    "\n",
    "*Synchroniser proprement deux sources temporelles (regard + vidéo), puis produire une vidéo annotée qui visualise, à chaque instant, la position exacte du regard dans la scène.*  \n",
    "Cette orchestration prépare le terrain pour toutes les étapes suivantes : segmentation d’affiches, homographies, heatmaps, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0427c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             section id                          recording id  \\\n",
      "0  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "1  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "2  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "3  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "4  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "\n",
      "        timestamp [ns]  gaze x [px]  gaze y [px]  worn  fixation id  blink id  \\\n",
      "0  1763456864828816499      435.227      576.743     1          1.0       NaN   \n",
      "1  1763456864833816499      445.815      577.747     1          1.0       NaN   \n",
      "2  1763456864838816499      447.881      574.315     1          1.0       NaN   \n",
      "3  1763456864843816499      449.868      571.571     1          1.0       NaN   \n",
      "4  1763456864848816499      453.487      572.783     1          1.0       NaN   \n",
      "\n",
      "   azimuth [deg]  elevation [deg]  \n",
      "0     -23.358268         0.851166  \n",
      "1     -22.678164         0.790091  \n",
      "2     -22.546188         1.006156  \n",
      "3     -22.419333         1.179084  \n",
      "4     -22.186604         1.103690  \n",
      "                             section id                          recording id  \\\n",
      "0  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "1  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "2  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "3  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "4  e0b2c246-195c-41f2-a3eb-f387f9f86d9a  42e0d11a-5676-4755-bf47-0bbe4d84aceb   \n",
      "\n",
      "        timestamp [ns]  \n",
      "0  1763456862813000000  \n",
      "1  1763456862863000000  \n",
      "2  1763456862913000000  \n",
      "3  1763456862963000000  \n",
      "4  1763456863013000000  \n",
      "        timestamp [ns]  gaze x [px]  gaze y [px]\n",
      "0  1763456864828816499      435.227      576.743\n",
      "1  1763456864833816499      445.815      577.747\n",
      "2  1763456864838816499      447.881      574.315\n",
      "3  1763456864843816499      449.868      571.571\n",
      "4  1763456864848816499      453.487      572.783\n",
      "        timestamp [ns]\n",
      "0  1763456862813000000\n",
      "1  1763456862863000000\n",
      "2  1763456862913000000\n",
      "3  1763456862963000000\n",
      "4  1763456863013000000\n",
      "        timestamp [ns]  gaze x [px]  gaze y [px]\n",
      "0  1763456862813000000          NaN          NaN\n",
      "1  1763456862863000000          NaN          NaN\n",
      "2  1763456862913000000          NaN          NaN\n",
      "3  1763456862963000000          NaN          NaN\n",
      "4  1763456863013000000          NaN          NaN\n",
      "Ouverture de la vidéo...\n",
      "Vidéo : 4106 frames, 29.75 fps, taille = 1600x1200\n",
      "Début du traitement des frames...\n",
      "Vidéo avec regard enregistrée dans: world_with_gaze_Sujet1.mp4\n"
     ]
    }
   ],
   "source": [
    "gaze = pd.read_csv(GAZE_CSV) # C'est les données de regard issus de la caméra interne de l'eye-tracker\n",
    "world_ts = pd.read_csv(WORLD_TS_CSV) # C'est les timestamps de chaque frame de la vidéo POV \n",
    "\n",
    "#Ici on va affichier les heads de gaze et world_ts \n",
    "print(gaze.head())\n",
    "print(world_ts.head())\n",
    "\n",
    "gaze = gaze[[\"timestamp [ns]\", \"gaze x [px]\", \"gaze y [px]\"]].copy()\n",
    "world_ts = world_ts[[\"timestamp [ns]\"]].copy()\n",
    "\n",
    "print(gaze.head())\n",
    "print(world_ts.head())\n",
    "\n",
    "\n",
    "frame_times = world_ts[\"timestamp [ns]\"].values # Ici on va se caler le tableau de tous les timestamps vidéo.\n",
    "frame_intervals = np.diff(frame_times) # Là calcule la différence entre chaque timestamp → on a les intervalles entre frames.\n",
    "median_dt = np.median(frame_intervals)  # en nanosecondes\n",
    "# tolérance = la moitié d'une période de frame\n",
    "tolerance_ns = int(median_dt/2)\n",
    "\n",
    "# merge_asof associe à chaque timestamp de frame l'échantillon de gaze le plus proche\n",
    "merged = pd.merge_asof(\n",
    "    world_ts,\n",
    "    gaze,\n",
    "    on=\"timestamp [ns]\",\n",
    "    direction=\"nearest\",\n",
    "    tolerance=tolerance_ns\n",
    ")\n",
    "# merged contient maintenant pour chaque frame :\n",
    "# - timestamp [ns]\n",
    "# - gaze x [px]\n",
    "# - gaze y [px] (NaN si aucun gaze assez proche dans la tolérance)\n",
    "print(merged.head())\n",
    "\n",
    "\n",
    "# Lecture de la vidéo + overlay des points\n",
    "print(\"Ouverture de la vidéo...\")\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Impossible d'ouvrir la vidéo: {VIDEO_PATH}\")\n",
    "\n",
    "# Récupération infos vidéo\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # ici fps représente le nombre de frames par seconde de la vidéo\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # ici width représente la largeur de la vidéo en pixels\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # ici height représente la hauteur de la vidéo en pixels\n",
    "\n",
    "print(f\"Vidéo : {len(world_ts)} frames, {fps:.2f} fps, taille = {width}x{height}\")\n",
    "\n",
    "\n",
    "# Préparation de la vidéo de sortie\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "print(\"Début du traitement des frames...\")\n",
    "for i in range(len(world_ts)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Fin de lecture vidéo à la frame {i}\")\n",
    "        break\n",
    "\n",
    "    row = merged.iloc[i]\n",
    "    x = row[\"gaze x [px]\"]\n",
    "    y = row[\"gaze y [px]\"]\n",
    "\n",
    "    # Si on a bien un point de regard pour cette frame\n",
    "    if not (np.isnan(x) or np.isnan(y)):\n",
    "        # Conversion en int, clamp pour éviter de sortir de l'image\n",
    "        px = int(np.clip(x, 0, width - 1))\n",
    "        py = int(np.clip(y, 0, height - 1))\n",
    "\n",
    "        # Dessiner un cercle rouge (BGR = (0, 0, 255))\n",
    "        cv2.circle(frame, (px, py), radius=10, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Vidéo avec regard enregistrée dans: {OUTPUT_VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eff24",
   "metadata": {},
   "source": [
    "### Objectif général du script\n",
    "\n",
    "Ce module applique une **correction de distorsion** à une vidéo à partir des paramètres de calibration contenus dans un fichier JSON.  \n",
    "Il constitue une étape préalable indispensable avant :\n",
    "\n",
    "- la détection des posters,\n",
    "- le recalage géométrique par homographie,\n",
    "- la projection du regard dans le repère scène,\n",
    "- la génération des heatmaps.\n",
    "\n",
    "Corriger la distorsion **avant tout traitement géométrique** garantit une scène cohérente, améliore la stabilité des homographies et supprime les déformations optiques du fish-eye.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Chargement de la calibration (`load_calibration()`)\n",
    "\n",
    "Cette fonction :\n",
    "\n",
    "- ouvre le fichier JSON généré par l’eye-tracker,\n",
    "- extrait la **matrice intrinsèque K** (fx, fy, cx, cy),\n",
    "- récupère les **coefficients de distorsion** (radiaux + tangentiel),\n",
    "- convertit l’ensemble en `numpy.float32` afin d’être compatible avec OpenCV.\n",
    "\n",
    "Ce chargement constitue la base nécessaire au modèle de projection pinhole + distorsion utilisé dans OpenCV.  \n",
    "Le vecteur de distorsion peut avoir **4, 5 ou 8 coefficients** — le script les conserve tous pour un maximum de fidélité.\n",
    "\n",
    "En sortie, la fonction renvoie :\n",
    "\n",
    "- **K** : matrice 3×3 de la caméra  \n",
    "- **dist** : vecteur (n×1) des coefficients de distorsion\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Ouverture de la vidéo et lecture des propriétés\n",
    "\n",
    "Dans `undistort_video()`, le script :\n",
    "\n",
    "- ouvre la vidéo d’entrée,\n",
    "- lit ses propriétés essentielles :\n",
    "  - résolution `(width, height)`\n",
    "  - nombre total de frames\n",
    "  - FPS\n",
    "\n",
    "Ces informations servent à :\n",
    "\n",
    "- dimensionner correctement la matrice optimisée,\n",
    "- initialiser la vidéo de sortie,\n",
    "- afficher une progression propre pendant le traitement.\n",
    "\n",
    "Un contrôle d’erreur est appliqué dès l’ouverture pour éviter un traitement silencieux sur un fichier inexistant.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Calcul de la matrice caméra optimisée (`getOptimalNewCameraMatrix`)\n",
    "\n",
    "OpenCV génère une nouvelle matrice intrinsèque tenant compte de la distorsion, qui permet :\n",
    "\n",
    "- de **réduire les bords noirs**,  \n",
    "- ou de **conserver le champ de vision complet**.\n",
    "\n",
    "Le paramètre **ALPHA** gouverne ce compromis :\n",
    "\n",
    "- **0.0 → crop maximum**, moins de bords noirs  \n",
    "- **1.0 → champ complet**, mais davantage de zones noires\n",
    "\n",
    "La fonction retourne aussi un **ROI** utile si l’on souhaite recadrer strictement la zone valide après correction.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Pré-calcul des cartes de remapping (`initUndistortRectifyMap`)\n",
    "\n",
    "Cette étape prépare deux cartes :\n",
    "\n",
    "- `map1` : coordonnées x corrigées  \n",
    "- `map2` : coordonnées y corrigées  \n",
    "\n",
    "Elles permettent d’appliquer la correction optique avec un simple appel à `cv2.remap()` sur chaque frame.  \n",
    "L’utilisation de `cv2.CV_16SC2` permet d’optimiser la mémoire et la vitesse d’exécution.\n",
    "\n",
    "Ce pré-calcul évite de recalculer la géométrie pour chaque image → **gain de performance majeur**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Traitement frame-par-frame\n",
    "\n",
    "Pour chaque image :\n",
    "\n",
    "1. Lecture de la frame brute  \n",
    "2. Correction via : cv2.remap(frame, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "3. Écriture dans la vidéo corrigée\n",
    "\n",
    "Un log est affiché toutes les **50 frames** pour assurer un suivi propre de la progression.\n",
    "\n",
    "Une option commentée permet de croper strictement à la zone valide fournie par le ROI.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Finalisation\n",
    "\n",
    "À la fin du traitement :\n",
    "\n",
    "- la vidéo d’entrée est libérée,  \n",
    "- la vidéo de sortie est fermée proprement,  \n",
    "- un message indique l’emplacement de la vidéo corrigée.\n",
    "\n",
    "Le résultat est une vidéo **rectifiée**, de même taille et même fps que l’originale, prête pour les étapes suivantes :  \n",
    "ORB → homographie → regard → heatmaps.\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé du pipeline\n",
    "\n",
    "1. Charger calibration → **K, dist**  \n",
    "2. Ouvrir vidéo → **fps, taille, frame_count**  \n",
    "3. Optimiser la matrice caméra → **new_K, ROI**  \n",
    "4. Générer les cartes → **map1, map2**  \n",
    "5. Corriger chaque frame → **remap()**  \n",
    "6. Exporter la vidéo corrigée  \n",
    "\n",
    "Pipeline fiable, performant, et totalement compatible avec le reste du projet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e38e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice caméra K :\n",
      " [[884.9203   0.     797.3327]\n",
      " [  0.     884.8444 590.4394]\n",
      " [  0.       0.       1.    ]]\n",
      "Coefficients de distorsion : [-0.1284729   0.10799497 -0.00076467  0.00041167  0.00231511  0.1688897\n",
      "  0.05152452  0.02927154]\n",
      "Vidéo d'entrée : 6838 frames, 29.82 fps, taille = 1600x1200\n",
      "Matrice caméra optimisée new_K :\n",
      " [[628.91     0.     798.5339]\n",
      " [  0.     749.0824 584.9927]\n",
      " [  0.       0.       1.    ]]\n",
      "ROI : (0, 0, 1599, 1199)\n",
      "50/6838 frames traitées...\n",
      "100/6838 frames traitées...\n",
      "150/6838 frames traitées...\n",
      "200/6838 frames traitées...\n",
      "250/6838 frames traitées...\n",
      "300/6838 frames traitées...\n",
      "350/6838 frames traitées...\n",
      "400/6838 frames traitées...\n",
      "450/6838 frames traitées...\n",
      "500/6838 frames traitées...\n",
      "550/6838 frames traitées...\n",
      "600/6838 frames traitées...\n",
      "650/6838 frames traitées...\n",
      "700/6838 frames traitées...\n",
      "750/6838 frames traitées...\n",
      "800/6838 frames traitées...\n",
      "850/6838 frames traitées...\n",
      "900/6838 frames traitées...\n",
      "950/6838 frames traitées...\n",
      "1000/6838 frames traitées...\n",
      "1050/6838 frames traitées...\n",
      "1100/6838 frames traitées...\n",
      "1150/6838 frames traitées...\n",
      "1200/6838 frames traitées...\n",
      "1250/6838 frames traitées...\n",
      "1300/6838 frames traitées...\n",
      "1350/6838 frames traitées...\n",
      "1400/6838 frames traitées...\n",
      "1450/6838 frames traitées...\n",
      "1500/6838 frames traitées...\n",
      "1550/6838 frames traitées...\n",
      "1600/6838 frames traitées...\n",
      "1650/6838 frames traitées...\n",
      "1700/6838 frames traitées...\n",
      "1750/6838 frames traitées...\n",
      "1800/6838 frames traitées...\n",
      "1850/6838 frames traitées...\n",
      "1900/6838 frames traitées...\n",
      "1950/6838 frames traitées...\n",
      "2000/6838 frames traitées...\n",
      "2050/6838 frames traitées...\n",
      "2100/6838 frames traitées...\n",
      "2150/6838 frames traitées...\n",
      "2200/6838 frames traitées...\n",
      "2250/6838 frames traitées...\n",
      "2300/6838 frames traitées...\n",
      "2350/6838 frames traitées...\n",
      "2400/6838 frames traitées...\n",
      "2450/6838 frames traitées...\n",
      "2500/6838 frames traitées...\n",
      "2550/6838 frames traitées...\n",
      "2600/6838 frames traitées...\n",
      "2650/6838 frames traitées...\n",
      "2700/6838 frames traitées...\n",
      "2750/6838 frames traitées...\n",
      "2800/6838 frames traitées...\n",
      "2850/6838 frames traitées...\n",
      "2900/6838 frames traitées...\n",
      "2950/6838 frames traitées...\n",
      "3000/6838 frames traitées...\n",
      "3050/6838 frames traitées...\n",
      "3100/6838 frames traitées...\n",
      "3150/6838 frames traitées...\n",
      "3200/6838 frames traitées...\n",
      "3250/6838 frames traitées...\n",
      "3300/6838 frames traitées...\n",
      "3350/6838 frames traitées...\n",
      "3400/6838 frames traitées...\n",
      "3450/6838 frames traitées...\n",
      "3500/6838 frames traitées...\n",
      "3550/6838 frames traitées...\n",
      "3600/6838 frames traitées...\n",
      "3650/6838 frames traitées...\n",
      "3700/6838 frames traitées...\n",
      "3750/6838 frames traitées...\n",
      "3800/6838 frames traitées...\n",
      "3850/6838 frames traitées...\n",
      "3900/6838 frames traitées...\n",
      "3950/6838 frames traitées...\n",
      "4000/6838 frames traitées...\n",
      "4050/6838 frames traitées...\n",
      "4100/6838 frames traitées...\n",
      "4150/6838 frames traitées...\n",
      "4200/6838 frames traitées...\n",
      "4250/6838 frames traitées...\n",
      "4300/6838 frames traitées...\n",
      "4350/6838 frames traitées...\n",
      "4400/6838 frames traitées...\n",
      "4450/6838 frames traitées...\n",
      "4500/6838 frames traitées...\n",
      "4550/6838 frames traitées...\n",
      "4600/6838 frames traitées...\n",
      "4650/6838 frames traitées...\n",
      "4700/6838 frames traitées...\n",
      "4750/6838 frames traitées...\n",
      "4800/6838 frames traitées...\n",
      "4850/6838 frames traitées...\n",
      "4900/6838 frames traitées...\n",
      "4950/6838 frames traitées...\n",
      "5000/6838 frames traitées...\n",
      "5050/6838 frames traitées...\n",
      "5100/6838 frames traitées...\n",
      "5150/6838 frames traitées...\n",
      "5200/6838 frames traitées...\n",
      "5250/6838 frames traitées...\n",
      "5300/6838 frames traitées...\n",
      "5350/6838 frames traitées...\n",
      "5400/6838 frames traitées...\n",
      "5450/6838 frames traitées...\n",
      "5500/6838 frames traitées...\n",
      "5550/6838 frames traitées...\n",
      "5600/6838 frames traitées...\n",
      "5650/6838 frames traitées...\n",
      "5700/6838 frames traitées...\n",
      "5750/6838 frames traitées...\n",
      "5800/6838 frames traitées...\n",
      "5850/6838 frames traitées...\n",
      "5900/6838 frames traitées...\n",
      "5950/6838 frames traitées...\n",
      "6000/6838 frames traitées...\n",
      "6050/6838 frames traitées...\n",
      "6100/6838 frames traitées...\n",
      "6150/6838 frames traitées...\n",
      "6200/6838 frames traitées...\n",
      "6250/6838 frames traitées...\n",
      "6300/6838 frames traitées...\n",
      "6350/6838 frames traitées...\n",
      "6400/6838 frames traitées...\n",
      "6450/6838 frames traitées...\n",
      "6500/6838 frames traitées...\n",
      "6550/6838 frames traitées...\n",
      "6600/6838 frames traitées...\n",
      "6650/6838 frames traitées...\n",
      "6700/6838 frames traitées...\n",
      "6750/6838 frames traitées...\n",
      "6800/6838 frames traitées...\n",
      "Vidéo corrigée enregistrée dans : world_undistorted_Sujet5.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# PARAMÈTRES À ADAPTER\n",
    "# -----------------------------\n",
    "CALIB_PATH   = Path(r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\AcquisitionsEyeTracker\\sujet5_m-671cf44e\\scene_camera.json\")  # fichier de calibration\n",
    "INPUT_VIDEO  = r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\Video_Eye_Tracking\\world_with_gaze_Sujet5.mp4\"              # ta vidéo d'entrée (avec ou sans gaze)\n",
    "OUTPUT_VIDEO = \"world_undistorted_Sujet5.mp4\"     # vidéo corrigée\n",
    "ALPHA        = 0.0                         # 0 = crop maximum des bords noirs, 1 = garde tout\n",
    "\n",
    "\n",
    "def load_calibration(calib_path: Path):\n",
    "    \"\"\"Charge la matrice caméra K et les coefficients de distorsion depuis le JSON.\"\"\"\n",
    "    with open(calib_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    K = np.array(data[\"camera_matrix\"], dtype=np.float32)\n",
    "    # OpenCV accepte un vecteur 4, 5 ou 8 pour la distorsion -> on garde tout\n",
    "    dist = np.array(data[\"distortion_coefficients\"], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    print(\"Matrice caméra K :\\n\", K)\n",
    "    print(\"Coefficients de distorsion :\", dist.ravel())\n",
    "    return K, dist\n",
    "\n",
    "\n",
    "def undistort_video(input_path: str, output_path: str, calib_path: Path, alpha: float = 0.0):\n",
    "    # 1. Charger la calibration\n",
    "    K, dist = load_calibration(calib_path)\n",
    "\n",
    "    # 2. Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Impossible d'ouvrir la vidéo : {input_path}\")\n",
    "\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Vidéo d'entrée : {frame_count} frames, {fps:.2f} fps, taille = {width}x{height}\")\n",
    "\n",
    "    # 3. Calcul de la nouvelle matrice caméra optimisée\n",
    "    #    ALPHA contrôle le compromis :\n",
    "    #    - 0.0 : moins de bords noirs, mais un léger crop\n",
    "    #    - 1.0 : conserve tout le champ de vision (plus de bords noirs)\n",
    "    new_K, roi = cv2.getOptimalNewCameraMatrix(K, dist, (width, height), alpha, (width, height))\n",
    "    print(\"Matrice caméra optimisée new_K :\\n\", new_K)\n",
    "    print(\"ROI :\", roi)\n",
    "\n",
    "    # 4. Pré-calcul des cartes de remapping (plus efficace que undistort frame par frame)\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(\n",
    "        K, dist, None, new_K, (width, height), cv2.CV_16SC2\n",
    "    )\n",
    "\n",
    "    # 5. Préparation de la vidéo de sortie (même taille et mêmes fps)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 6. Appliquer la correction de distorsion\n",
    "        undistorted = cv2.remap(frame, map1, map2, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # Si tu veux croper strictement à la zone valide, décommente :\n",
    "        # x, y, w, h = roi\n",
    "        # undistorted = undistorted[y:y+h, x:x+w]\n",
    "\n",
    "        out.write(undistorted)\n",
    "        frame_idx += 1\n",
    "\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"{frame_idx}/{frame_count} frames traitées...\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Vidéo corrigée enregistrée dans : {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    undistort_video(INPUT_VIDEO, OUTPUT_VIDEO, CALIB_PATH, alpha=ALPHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb293f77",
   "metadata": {},
   "source": [
    "### Objectif du script\n",
    "\n",
    "Ce module détecte automatiquement des **affiches** dans une vidéo grâce à ORB, au matching de descripteurs et à l’estimation d’homographies robustes.  \n",
    "Il sert de base pour les étapes suivantes : localisation des posters, association avec le regard, et génération de heatmaps.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Chargement des affiches\n",
    "\n",
    "La fonction `load_posters()` :\n",
    "\n",
    "* parcourt `POSTERS_DIR` et charge chaque image (.png/.jpg),\n",
    "* réduit sa taille si nécessaire (`MAX_POSTER_SIZE`) pour accélérer le matching,\n",
    "* extrait des keypoints + descripteurs ORB,\n",
    "* définit les **4 coins** du poster dans son repère image pour permettre sa projection dans la vidéo.\n",
    "\n",
    "Chaque affiche est stockée avec :\n",
    "\n",
    "* `name`,\n",
    "* `kp` / `des` (features ORB),\n",
    "* `corners` (rectangle de référence),\n",
    "* `image` (pour debug éventuel).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Détection dans une frame vidéo\n",
    "\n",
    "`detect_posters_in_frame(frame, posters, orb, bf)` effectue :\n",
    "\n",
    "1. Extraction ORB de la frame.\n",
    "2. Pour chaque poster :\n",
    "   * Matching KNN des descripteurs (poster → frame),\n",
    "   * Application du **Lowe ratio test** (`RATIO_TEST`),\n",
    "   * Rejet si peu de bons matches (`MIN_GOOD_MATCHES`),\n",
    "   * Calcul d’une **homographie** via `cv2.findHomography(..., RANSAC)`,\n",
    "   * Vérification du nombre d’inliers (`MIN_INLIERS`),\n",
    "   * Projection des coins du poster dans la frame via `cv2.perspectiveTransform`.\n",
    "\n",
    "Chaque détection retournée contient :\n",
    "\n",
    "* `name`,\n",
    "* `corners` projetés,\n",
    "* `inliers`,\n",
    "* `good_matches`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Dessin des détections\n",
    "\n",
    "`draw_detections()` :\n",
    "\n",
    "* trace un polygone vert autour du poster détecté,\n",
    "* affiche un label : `NomAffiche (nb_inliers)`.\n",
    "\n",
    "Cela permet de visualiser rapidement la qualité et la fiabilité de la détection.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Orchestration générale (`main()`)\n",
    "\n",
    "La fonction principale :\n",
    "\n",
    "1. Initialise ORB + BFMatcher.\n",
    "2. Charge toutes les affiches (features pré-calculées).\n",
    "3. Ouvre la vidéo (`VIDEO_PATH`) et lit ses propriétés (fps, taille).\n",
    "4. Crée une vidéo de sortie (`OUTPUT_VIDEO`).\n",
    "5. Pour chaque frame :\n",
    "   * détecte les affiches,\n",
    "   * dessine les cadres,\n",
    "   * écrit la frame annotée.\n",
    "6. Libère proprement les ressources.\n",
    "\n",
    "Le script affiche un log toutes les **50 frames** pour suivre la progression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35bff9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anning redimensionnée à 666x1000\n",
      "Affiche chargée : Anning, 2000 keypoints\n",
      "Barres redimensionnée à 666x1000\n",
      "Affiche chargée : Barres, 2000 keypoints\n",
      "Bell redimensionnée à 666x1000\n",
      "Affiche chargée : Bell, 2000 keypoints\n",
      "Bunten-Berry redimensionnée à 666x1000\n",
      "Affiche chargée : Bunten-Berry, 2000 keypoints\n",
      "Franklin redimensionnée à 666x1000\n",
      "Affiche chargée : Franklin, 2000 keypoints\n",
      "Gautier redimensionnée à 666x1000\n",
      "Affiche chargée : Gautier, 2000 keypoints\n",
      "Johnson redimensionnée à 666x1000\n",
      "Affiche chargée : Johnson, 2000 keypoints\n",
      "Noether redimensionnée à 666x1000\n",
      "Affiche chargée : Noether, 2000 keypoints\n",
      "Vidéo : 7388 frames, 29.82 fps, taille = 1600x1200\n",
      "50/7388 frames traitées...\n",
      "100/7388 frames traitées...\n",
      "150/7388 frames traitées...\n",
      "200/7388 frames traitées...\n",
      "250/7388 frames traitées...\n",
      "300/7388 frames traitées...\n",
      "350/7388 frames traitées...\n",
      "400/7388 frames traitées...\n",
      "450/7388 frames traitées...\n",
      "500/7388 frames traitées...\n",
      "550/7388 frames traitées...\n",
      "600/7388 frames traitées...\n",
      "650/7388 frames traitées...\n",
      "700/7388 frames traitées...\n",
      "750/7388 frames traitées...\n",
      "800/7388 frames traitées...\n",
      "850/7388 frames traitées...\n",
      "900/7388 frames traitées...\n",
      "950/7388 frames traitées...\n",
      "1000/7388 frames traitées...\n",
      "1050/7388 frames traitées...\n",
      "1100/7388 frames traitées...\n",
      "1150/7388 frames traitées...\n",
      "1200/7388 frames traitées...\n",
      "1250/7388 frames traitées...\n",
      "1300/7388 frames traitées...\n",
      "1350/7388 frames traitées...\n",
      "1400/7388 frames traitées...\n",
      "1450/7388 frames traitées...\n",
      "1500/7388 frames traitées...\n",
      "1550/7388 frames traitées...\n",
      "1600/7388 frames traitées...\n",
      "1650/7388 frames traitées...\n",
      "1700/7388 frames traitées...\n",
      "1750/7388 frames traitées...\n",
      "1800/7388 frames traitées...\n",
      "1850/7388 frames traitées...\n",
      "1900/7388 frames traitées...\n",
      "1950/7388 frames traitées...\n",
      "2000/7388 frames traitées...\n",
      "2050/7388 frames traitées...\n",
      "2100/7388 frames traitées...\n",
      "2150/7388 frames traitées...\n",
      "2200/7388 frames traitées...\n",
      "2250/7388 frames traitées...\n",
      "2300/7388 frames traitées...\n",
      "2350/7388 frames traitées...\n",
      "2400/7388 frames traitées...\n",
      "2450/7388 frames traitées...\n",
      "2500/7388 frames traitées...\n",
      "2550/7388 frames traitées...\n",
      "2600/7388 frames traitées...\n",
      "2650/7388 frames traitées...\n",
      "2700/7388 frames traitées...\n",
      "2750/7388 frames traitées...\n",
      "2800/7388 frames traitées...\n",
      "2850/7388 frames traitées...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 224\u001b[39m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Vidéo avec détection des affiches : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_VIDEO\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m detections = \u001b[43mdetect_posters_in_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m frame_out = draw_detections(frame, detections)\n\u001b[32m    212\u001b[39m out.write(frame_out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mdetect_posters_in_frame\u001b[39m\u001b[34m(frame, posters, orb, bf)\u001b[39m\n\u001b[32m    124\u001b[39m dst_pts = np.float32(\n\u001b[32m    125\u001b[39m     [kp_frame[m.trainIdx].pt \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m good]\n\u001b[32m    126\u001b[39m ).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Homographie robuste via RANSAC\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m H, mask = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindHomography\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRANSAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m H \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PARAMÈTRES GÉNÉRAUX\n",
    "# ---------------------------------------------------------\n",
    "# → Mets ici ta vidéo (idéalement la version undistordue)\n",
    "VIDEO_PATH   = r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\Video_Eye_Tracking\\VideoSansDistortion\\world_undistorted_Sujet3.mp4\"\n",
    "# → Dossier contenant les images des affiches (PNG ou JPG)\n",
    "POSTERS_DIR  = Path(r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\posters\")\n",
    "# → Vidéo de sortie avec les cadres autour des affiches détectées\n",
    "OUTPUT_VIDEO = r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\Video_Eye_Tracking\\ORB_testV1_Sujet3.mp4\"\n",
    "# ---------------------------------------------------------\n",
    "# PARAMÈTRES ORB / MATCHING / HOMOGRAPHIE\n",
    "# ---------------------------------------------------------\n",
    "MAX_POSTER_SIZE   = 1000   # taille max (en pixels) pour le plus grand côté de l'affiche\n",
    "ORB_NFEATURES     = 2000   # nombre max de keypoints ORB\n",
    "RATIO_TEST        = 0.75   # Lowe ratio test\n",
    "MIN_GOOD_MATCHES  = 25     # nombre mini de bons matches avant homographie\n",
    "MIN_INLIERS       = 25     # nombre mini d'inliers RANSAC pour valider une affiche\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHARGEMENT DES AFFICHES ET EXTRACTION DES FEATURES\n",
    "# ---------------------------------------------------------\n",
    "def load_posters(orb, posters_dir: Path):\n",
    "    \"\"\"\n",
    "    Charge toutes les affiches dans posters_dir, les redimensionne\n",
    "    si besoin, extrait les keypoints ORB et prépare les coins du rectangle\n",
    "    de référence pour la projection via homographie.\n",
    "    \"\"\"\n",
    "    posters = []\n",
    "\n",
    "    if not posters_dir.exists():\n",
    "        raise FileNotFoundError(f\"Dossier d'affiches introuvable : {posters_dir}\")\n",
    "\n",
    "    for img_path in posters_dir.glob(\"*.*\"):\n",
    "        if img_path.suffix.lower() not in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            print(f\"⚠ Impossible de lire {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Redimensionnement éventuel pour limiter la résolution\n",
    "        h, w = img.shape[:2]\n",
    "        scale = min(MAX_POSTER_SIZE / max(w, h), 1.0)  # <= 1.0 -> jamais agrandir\n",
    "        if scale < 1.0:\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            print(f\"{img_path.stem} redimensionnée à {new_w}x{new_h}\")\n",
    "        else:\n",
    "            print(f\"{img_path.stem} gardée à taille originale {w}x{h}\")\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = orb.detectAndCompute(gray, None)\n",
    "        h, w = gray.shape\n",
    "\n",
    "        # Coins du poster dans son repère image (rectangle)\n",
    "        corners = np.array(\n",
    "            [[0,   0],\n",
    "             [w-1, 0],\n",
    "             [w-1, h-1],\n",
    "             [0,   h-1]],\n",
    "            dtype=np.float32\n",
    "        ).reshape(-1, 1, 2)\n",
    "\n",
    "        posters.append({\n",
    "            \"name\": img_path.stem,\n",
    "            \"image\": img,\n",
    "            \"kp\": kp,\n",
    "            \"des\": des,\n",
    "            \"corners\": corners\n",
    "        })\n",
    "        print(f\"Affiche chargée : {img_path.stem}, {len(kp)} keypoints\")\n",
    "\n",
    "    return posters\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DÉTECTION D'UNE OU PLUSIEURS AFFICHES DANS UNE FRAME\n",
    "# ---------------------------------------------------------\n",
    "def detect_posters_in_frame(frame, posters, orb, bf):\n",
    "    \"\"\"\n",
    "    Retourne une liste de détections pour la frame :\n",
    "    [ { \"name\": str, \"corners\": corners_trans, \"inliers\": int, \"good_matches\": int }, ... ]\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = orb.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    if des_frame is None or len(kp_frame) == 0:\n",
    "        return detections\n",
    "\n",
    "    for poster in posters:\n",
    "        des_p = poster[\"des\"]\n",
    "        kp_p  = poster[\"kp\"]\n",
    "        if des_p is None or len(kp_p) == 0:\n",
    "            continue\n",
    "\n",
    "        # Matching KNN des descripteurs de l'affiche vers ceux de la frame\n",
    "        matches = bf.knnMatch(des_p, des_frame, k=2)\n",
    "\n",
    "        # Lowe ratio test\n",
    "        good = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < RATIO_TEST * n.distance:\n",
    "                good.append(m)\n",
    "\n",
    "        if len(good) < MIN_GOOD_MATCHES:\n",
    "            # Pas assez de correspondances pour tenter une homographie\n",
    "            continue\n",
    "\n",
    "        # Points source (affiche) et destination (frame)\n",
    "        src_pts = np.float32(\n",
    "            [kp_p[m.queryIdx].pt for m in good]\n",
    "        ).reshape(-1, 1, 2)\n",
    "\n",
    "        dst_pts = np.float32(\n",
    "            [kp_frame[m.trainIdx].pt for m in good]\n",
    "        ).reshape(-1, 1, 2)\n",
    "\n",
    "        # Homographie robuste via RANSAC\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if H is None:\n",
    "            continue\n",
    "\n",
    "        mask = mask.ravel().tolist()\n",
    "        inliers = sum(mask)\n",
    "\n",
    "        if inliers < MIN_INLIERS:\n",
    "            continue\n",
    "\n",
    "        # Projection des coins du poster dans la frame\n",
    "        corners_trans = cv2.perspectiveTransform(poster[\"corners\"], H)\n",
    "\n",
    "        detections.append({\n",
    "            \"name\": poster[\"name\"],\n",
    "            \"corners\": corners_trans,\n",
    "            \"inliers\": inliers,\n",
    "            \"good_matches\": len(good)\n",
    "        })\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# DESSIN DES RÉSULTATS SUR LA FRAME\n",
    "# ---------------------------------------------------------\n",
    "def draw_detections(frame, detections):\n",
    "    \"\"\"\n",
    "    Dessine les cadres + labels des affiches détectées sur la frame.\n",
    "    \"\"\"\n",
    "    for det in detections:\n",
    "        pts = det[\"corners\"].astype(int)\n",
    "\n",
    "        # tracer le contour du poster\n",
    "        cv2.polylines(frame, [pts], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "\n",
    "        # texte (nom + nb d'inliers)\n",
    "        x, y = pts[0, 0, 0], pts[0, 0, 1]\n",
    "        label = f\"{det['name']} ({det['inliers']} inliers)\"\n",
    "        cv2.putText(frame, label, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ORCHESTRATION GLOBALE\n",
    "# ---------------------------------------------------------\n",
    "def main():\n",
    "    # ORB + BFMatcher\n",
    "    orb = cv2.ORB_create(nfeatures=ORB_NFEATURES)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "\n",
    "    # 1. Charger les affiches\n",
    "    posters = load_posters(orb, POSTERS_DIR)\n",
    "    if not posters:\n",
    "        raise RuntimeError(\"Aucune affiche chargée, vérifie le dossier POSTERS_DIR.\")\n",
    "\n",
    "    # 2. Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Impossible d'ouvrir la vidéo {VIDEO_PATH}\")\n",
    "\n",
    "    fps         = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width       = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height      = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Vidéo : {frame_count} frames, {fps:.2f} fps, taille = {width}x{height}\")\n",
    "\n",
    "    # 3. Préparer la vidéo de sortie\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (width, height))\n",
    "\n",
    "    # 4. Boucle frame par frame\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        detections = detect_posters_in_frame(frame, posters, orb, bf)\n",
    "        frame_out = draw_detections(frame, detections)\n",
    "        out.write(frame_out)\n",
    "\n",
    "        frame_idx += 1\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"{frame_idx}/{frame_count} frames traitées...\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"✅ Vidéo avec détection des affiches : {OUTPUT_VIDEO}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6082b",
   "metadata": {},
   "source": [
    "### Objectif général du module\n",
    "\n",
    "Ce script réalise l’ensemble du pipeline **détection d’affiches → association avec le regard → génération de heatmaps**, en travaillant directement à partir :\n",
    "\n",
    "- des vidéos corrigées de l’eye-tracker,\n",
    "- des CSV gaze + timestamps monde,\n",
    "- des images d’affiches,\n",
    "- et des matrices caméra/distorsion pour corriger les points de regard.\n",
    "\n",
    "L’objectif final est de produire **une heatmap par affiche**, représentant la distribution spatiale du regard projeté dans le repère de l’affiche (homographie inverse).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Paramètres globaux & calibration caméra\n",
    "\n",
    "La première section déclare :\n",
    "\n",
    "- les chemins vers la vidéo, les CSV (gaze + world timestamps), et le dossier des affiches,\n",
    "- les hyperparamètres ORB / matching / homographie,\n",
    "- la résolution des heatmaps,\n",
    "- les paramètres intrinsèques caméra (fx, fy, cx, cy) et la distorsion (8 coefficients de l’eye-tracker).\n",
    "\n",
    "Ces éléments servent à :\n",
    "\n",
    "- normaliser les points de regard via `cv2.undistortPoints`,\n",
    "- assurer la cohérence géométrique entre la vidéo, les projections et les homographies,\n",
    "- dimensionner correctement les heatmaps (downscale configurable).\n",
    "\n",
    "On affiche aussi K et le vecteur de distorsion pour vérifier que la calibration est correctement chargée.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fusion regard ⇄ timestamps monde (avec undistortion)\n",
    "\n",
    "`build_merged_gaze_world()` fusionne les données de regard avec la timeline vidéo grâce à `merge_asof`.\n",
    "\n",
    "Pipeline :\n",
    "\n",
    "1. **Chargement des CSV gaze + world timestamps**  \n",
    "   On extrait uniquement les colonnes nécessaires.\n",
    "\n",
    "2. **Correction optique du regard**  \n",
    "   On applique : cv2.undistortPoints(pts, K, dist, P=new_camera_matrix)\n",
    "   \n",
    "→ conversion des coordonnées distordues en coordonnées **undistordues dans le même repère que la vidéo corrigée**.\n",
    "\n",
    "3. **Tolérance temporelle automatique**  \n",
    "On estime la période médiane entre frames vidéo pour définir la tolérance d’alignement gaze ↔ frame.\n",
    "\n",
    "4. **Fusion finalisée**  \n",
    "Chaque frame vidéo possède désormais une valeur gaze (x, y) interpolée temporellement.\n",
    "\n",
    "Cette fusion garantit que chaque frame peut être traitée avec un regard cohérent, même en cas de petites variations de timestamp.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Suppression des doublons via NMS\n",
    "\n",
    "L’algorithme NMS maison permet d’éviter d’avoir plusieurs détections du même poster dans une frame.\n",
    "\n",
    "Détails :\n",
    "\n",
    "- `bbox_from_corners()` convertit les coins transformés en boîte axis-aligned.\n",
    "- `iou()` calcule l’intersection sur union entre deux boîtes.\n",
    "- `suppress_overlaps()` :\n",
    "- trie les détections par score (nb d’inliers),\n",
    "- supprime les doublons selon un `iou_thresh`,\n",
    "- ne conserve que les posters les plus fiables.\n",
    "\n",
    "Le NMS garantit que chaque poster est représenté **une seule fois par frame**, ce qui rend la heatmap plus propre.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Chargement des affiches & extraction ORB\n",
    "\n",
    "`load_posters()` :\n",
    "\n",
    "1. Charge chaque image d’affiche depuis `POSTERS_DIR`.\n",
    "2. Réduit sa taille si nécessaire (`MAX_POSTER_SIZE`) pour accélérer ORB.\n",
    "3. Convertit en niveau de gris, extrait :\n",
    "- les keypoints,\n",
    "- les descripteurs ORB.\n",
    "4. Définit les **coins de référence** du poster :  [0,0], [w-1,0], [w-1,h-1], [0,h-1]\n",
    "5. Stocke chaque poster dans une structure contenant :\n",
    "- `name`\n",
    "- `img`\n",
    "- `kp` / `des`\n",
    "- `corners`\n",
    "- `size`\n",
    "\n",
    "Les posters ainsi pré-traités peuvent être directement utilisés pour le matching dans chaque frame vidéo.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Détection des posters dans une frame vidéo\n",
    "\n",
    "`detect_posters_in_frame()` effectue :\n",
    "\n",
    "#### 1) Extraction des features de la frame\n",
    "- conversion en gris,\n",
    "- ORB.detectAndCompute.\n",
    "\n",
    "#### 2) Matching KNN poster → frame\n",
    "On utilise un BFMatcher Hamming :\n",
    "- ratio test Lowe (`RATIO_TEST`),\n",
    "- filtrage des mauvais matches.\n",
    "\n",
    "#### 3) Homographie RANSAC\n",
    "Si assez de matches :\n",
    "- estimation H = `cv2.findHomography`,\n",
    "- comptage des inliers,\n",
    "- rejet si `< MIN_INLIERS`.\n",
    "\n",
    "#### 4) Projection du rectangle du poster\n",
    "On transforme les coins avec : cv2.perspectiveTransform(poster[\"corners\"], H)\n",
    "\n",
    "#### 5) Application du NMS\n",
    "On retourne une liste nettoyée de détections valides.\n",
    "\n",
    "Chaque détection contient :\n",
    "- nom du poster,\n",
    "- coins projetés,\n",
    "- nb d’inliers,\n",
    "- nb de bons matches,\n",
    "- matrice H,\n",
    "- taille du poster.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Initialisation & mise à jour des heatmaps\n",
    "\n",
    "#### `init_heatmaps()`\n",
    "Crée une heatmap vide (float32) pour chaque poster, avec une résolution réduite (`HEATMAP_DOWNSCALE`).\n",
    "\n",
    "#### `update_heatmaps()`\n",
    "Pour chaque detection :\n",
    "\n",
    "1. Test si le regard se situe **dans le quadrilatère projeté** (pointPolygonTest).\n",
    "2. Conversion du regard frame → poster via l’homographie inverse : uv = H_inv * [gx, gy]\n",
    "\n",
    "3. Normalisation dans le repère de la heatmap (u/w, v/h).\n",
    "4. Incrément de la cellule correspondante.\n",
    "\n",
    "Ce mécanisme associe chaque regard à sa position exacte dans le poster, indépendamment de la perspective.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Génération & sauvegarde des heatmaps\n",
    "\n",
    "`save_heatmaps()` :\n",
    "\n",
    "1. Lisse la heatmap via un Gaussian blur.\n",
    "2. Normalise et convertit en image 8 bits.\n",
    "3. Colorise (`COLORMAP_JET`) puis upscale à la taille de l’affiche.\n",
    "4. Fusionne l’affiche + heatmap via alpha blending.\n",
    "5. Sauvegarde le fichier final sous forme PNG.\n",
    "\n",
    "Chaque poster reçoit son image de chaleur finale.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Pipeline principal (`main()`)\n",
    "\n",
    "La boucle globale :\n",
    "\n",
    "1. Construit le merged gaze/world.\n",
    "2. Initialise ORB + matcher.\n",
    "3. Charge les affiches.\n",
    "4. Initialise les heatmaps.\n",
    "5. Parcourt la vidéo frame par frame :\n",
    "- détecte les posters,\n",
    "- récupérer gaze_x, gaze_y alignés,\n",
    "- met à jour la heatmap correspondante.\n",
    "6. Affiche un log toutes les 50 frames.\n",
    "7. Enregistre toutes les heatmaps à la fin.\n",
    "\n",
    "Aucune vidéo n’est générée : seule la **carte thermique finale** pour chaque poster est produite.\n",
    "\n",
    "---\n",
    "\n",
    "### Résumé global du pipeline\n",
    "\n",
    "1. Correction du regard (undistortion)  \n",
    "2. Fusion temporelle gaze ↔ vidéo  \n",
    "3. Extraction ORB affiches + frame  \n",
    "4. Matching + ratio test  \n",
    "5. Homographie RANSAC + projection  \n",
    "6. NMS pour éviter les doublons  \n",
    "7. Homographie inverse pour projeter le regard  \n",
    "8. Génération des heatmaps lissées  \n",
    "9. Sauvegarde finale poster + heatmap\n",
    "\n",
    "Pipeline complet, propre et robuste pour analyser très précisément la distribution du regard sur chaque affiche.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13da827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =\\n [[884.9203   0.     797.3327]\n",
      " [  0.     884.8444 590.4394]\n",
      " [  0.       0.       1.    ]]\n",
      "dist (8) = [-0.1284729   0.10799497 -0.00076467  0.00041167  0.00231511  0.1688897\n",
      "  0.05152452  0.02927154]\n",
      "dist (opencv 5) = [-0.1284729   0.10799497 -0.00076467  0.00041167  0.00231511]\n",
      "Affiche chargée : Anning (2000 keypoints)\n",
      "Affiche chargée : Barres (2000 keypoints)\n",
      "Affiche chargée : Bell (2000 keypoints)\n",
      "Affiche chargée : Bunten-Berry (2000 keypoints)\n",
      "Affiche chargée : Franklin (2000 keypoints)\n",
      "Affiche chargée : Gautier (2000 keypoints)\n",
      "Affiche chargée : Johnson (2000 keypoints)\n",
      "Affiche chargée : Noether (2000 keypoints)\n",
      "Début du traitement des frames (heatmaps uniquement)...\n",
      "50 frames traitées...\n",
      "100 frames traitées...\n",
      "150 frames traitées...\n",
      "200 frames traitées...\n",
      "250 frames traitées...\n",
      "300 frames traitées...\n",
      "350 frames traitées...\n",
      "400 frames traitées...\n",
      "450 frames traitées...\n",
      "500 frames traitées...\n",
      "550 frames traitées...\n",
      "600 frames traitées...\n",
      "650 frames traitées...\n",
      "700 frames traitées...\n",
      "750 frames traitées...\n",
      "800 frames traitées...\n",
      "850 frames traitées...\n",
      "900 frames traitées...\n",
      "950 frames traitées...\n",
      "1000 frames traitées...\n",
      "1050 frames traitées...\n",
      "1100 frames traitées...\n",
      "1150 frames traitées...\n",
      "1200 frames traitées...\n",
      "1250 frames traitées...\n",
      "1300 frames traitées...\n",
      "1350 frames traitées...\n",
      "1400 frames traitées...\n",
      "1450 frames traitées...\n",
      "1500 frames traitées...\n",
      "1550 frames traitées...\n",
      "1600 frames traitées...\n",
      "1650 frames traitées...\n",
      "1700 frames traitées...\n",
      "1750 frames traitées...\n",
      "1800 frames traitées...\n",
      "1850 frames traitées...\n",
      "1900 frames traitées...\n",
      "1950 frames traitées...\n",
      "2000 frames traitées...\n",
      "2050 frames traitées...\n",
      "2100 frames traitées...\n",
      "Fin du parcours vidéo, génération des heatmaps…\n",
      "✅ Heatmap sauvegardée pour Anning : Sujet4_heatmap_Anning_V3.png\n",
      "✅ Heatmap sauvegardée pour Barres : Sujet4_heatmap_Barres_V3.png\n",
      "✅ Heatmap sauvegardée pour Bell : Sujet4_heatmap_Bell_V3.png\n",
      "✅ Heatmap sauvegardée pour Bunten-Berry : Sujet4_heatmap_Bunten-Berry_V3.png\n",
      "✅ Heatmap sauvegardée pour Franklin : Sujet4_heatmap_Franklin_V3.png\n",
      "✅ Heatmap sauvegardée pour Gautier : Sujet4_heatmap_Gautier_V3.png\n",
      "✅ Heatmap sauvegardée pour Johnson : Sujet4_heatmap_Johnson_V3.png\n",
      "✅ Heatmap sauvegardée pour Noether : Sujet4_heatmap_Noether_V3.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# PATHS À ADAPTER\n",
    "# ============================================================\n",
    "\n",
    "VIDEO_PATH = r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\Video_Eye_Tracking\\ORB_testV3_Sujet4.mp4\"\n",
    "GAZE_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet4_m-fee537df\\\\gaze.csv\"\n",
    "WORLD_TS_CSV = \"C:\\\\Users\\\\yanis\\\\Downloads\\\\yanisVsCode\\\\APP\\\\AcquisitionsEyeTracker\\\\sujet4_m-fee537df\\\\world_timestamps.csv\"\n",
    "POSTERS_DIR     = Path(r\"C:\\Users\\yanis\\Downloads\\yanisVsCode\\APP\\posters\")\n",
    "\n",
    "# résolution de la heatmap (réduction par rapport à l’affiche)\n",
    "HEATMAP_DOWNSCALE   = 8\n",
    "GAUSSIAN_BLUR_KSIZE = (35, 35)\n",
    "\n",
    "# ORB / matching / homographie\n",
    "MAX_POSTER_SIZE  = 1000\n",
    "ORB_NFEATURES    = 2000\n",
    "RATIO_TEST       = 0.70\n",
    "MIN_GOOD_MATCHES = 20\n",
    "MIN_INLIERS      = 20\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CALIBRATION CAMÉRA  (dépendant de eye-tracker)\n",
    "# ============================================================\n",
    "\n",
    "fx = 884.9202751251051\n",
    "fy = 884.8443962022465\n",
    "cx = 797.3326895814895\n",
    "cy = 590.4393801156965\n",
    "\n",
    "camera_matrix = np.array([\n",
    "    [fx, 0.0, cx],\n",
    "    [0.0, fy, cy],\n",
    "    [0.0, 0.0, 1.0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Distorsion fournie (8 coefficients dans vos JSON)\n",
    "dist_coeffs = np.array([\n",
    "    -0.12847289024405859,\n",
    "     0.10799496401849330,\n",
    "    -0.000764670173249848,\n",
    "     0.0004116699644771188,\n",
    "     0.0023151069375056533,\n",
    "     0.168889696495412,\n",
    "     0.0515245213892395,\n",
    "     0.029271541722755288\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Si tu utilises la convention OpenCV (k1,k2,p1,p2,k3) tu peux prendre les 5 premiers éléments :\n",
    "dist_coeffs_opencv5 = dist_coeffs[:5].copy()\n",
    "\n",
    "# new_camera_matrix : utiliser la même si tu n'as pas recalculé newK via getOptimalNewCameraMatrix\n",
    "new_camera_matrix = camera_matrix.copy()\n",
    "\n",
    "# Affichage rapide pour vérification\n",
    "print('K =\\\\n', camera_matrix)\n",
    "print('dist (8) =', dist_coeffs)\n",
    "print('dist (opencv 5) =', dist_coeffs_opencv5)\n",
    "\n",
    "# ============================================================\n",
    "# MERGE GAZE / WORLD EN TENANT COMPTE DE L’UNDISTORTION\n",
    "# ============================================================\n",
    "\n",
    "def build_merged_gaze_world():\n",
    "    gaze = pd.read_csv(GAZE_CSV)\n",
    "    world_ts = pd.read_csv(WORLD_TS_CSV)\n",
    "\n",
    "    gaze = gaze[[\"timestamp [ns]\", \"gaze x [px]\", \"gaze y [px]\"]].copy()\n",
    "    world_ts = world_ts[[\"timestamp [ns]\"]].copy()\n",
    "\n",
    "    # undistort des points de regard (coordonnées caméra distordue -> undistordue)\n",
    "    pts = gaze[[\"gaze x [px]\", \"gaze y [px]\"]].values.astype(np.float32).reshape(-1, 1, 2)\n",
    "    pts_undist = cv2.undistortPoints(pts, camera_matrix, dist_coeffs, P=new_camera_matrix)\n",
    "\n",
    "    gaze[\"gaze x [px]\"] = pts_undist[:, 0, 0]\n",
    "    gaze[\"gaze y [px]\"] = pts_undist[:, 0, 1]\n",
    "\n",
    "    # tolérance temporelle = moitié de la période entre deux frames vidéo\n",
    "    frame_times = world_ts[\"timestamp [ns]\"].values\n",
    "    frame_intervals = np.diff(frame_times)\n",
    "    median_dt = np.median(frame_intervals)         # ns\n",
    "    tolerance_ns = int(median_dt / 2)\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        world_ts.sort_values(\"timestamp [ns]\"),\n",
    "        gaze.sort_values(\"timestamp [ns]\"),\n",
    "        on=\"timestamp [ns]\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=tolerance_ns\n",
    "    )\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NMS POUR ÉVITER LES DOUBLONS DE CADRES\n",
    "# ============================================================\n",
    "\n",
    "def bbox_from_corners(corners: np.ndarray) -> np.ndarray:\n",
    "    pts = corners.reshape(-1, 2)\n",
    "    x_min = pts[:, 0].min()\n",
    "    y_min = pts[:, 1].min()\n",
    "    x_max = pts[:, 0].max()\n",
    "    y_max = pts[:, 1].max()\n",
    "    return np.array([x_min, y_min, x_max, y_max], dtype=np.float32)\n",
    "\n",
    "def iou(b1: np.ndarray, b2: np.ndarray) -> float:\n",
    "    xA = max(b1[0], b2[0])\n",
    "    yA = max(b1[1], b2[1])\n",
    "    xB = min(b1[2], b2[2])\n",
    "    yB = min(b1[3], b2[3])\n",
    "\n",
    "    inter_w = max(0.0, xB - xA)\n",
    "    inter_h = max(0.0, yB - yA)\n",
    "    inter   = inter_w * inter_h\n",
    "    if inter <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
    "    area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
    "    union = area1 + area2 - inter\n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "def suppress_overlaps(detections, iou_thresh: float = 0.3):\n",
    "    if not detections:\n",
    "        return []\n",
    "\n",
    "    for det in detections:\n",
    "        det[\"bbox\"]  = bbox_from_corners(det[\"corners\"])\n",
    "        det[\"score\"] = det[\"inliers\"]\n",
    "\n",
    "    dets = sorted(detections, key=lambda d: d[\"score\"], reverse=True)\n",
    "    kept = []\n",
    "    for det in dets:\n",
    "        keep = True\n",
    "        for k in kept:\n",
    "            if iou(det[\"bbox\"], k[\"bbox\"]) > iou_thresh:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            kept.append(det)\n",
    "\n",
    "    for det in kept:\n",
    "        det.pop(\"bbox\", None)\n",
    "        det.pop(\"score\", None)\n",
    "\n",
    "    return kept\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CHARGEMENT DES AFFICHES + ORB\n",
    "# ============================================================\n",
    "\n",
    "def load_posters(orb, posters_dir: Path):\n",
    "    posters = []\n",
    "    if not posters_dir.exists():\n",
    "        raise FileNotFoundError(f\"Dossier d'affiches introuvable : {posters_dir}\")\n",
    "\n",
    "    for img_path in posters_dir.glob(\"*\"):\n",
    "        if img_path.suffix.lower() not in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            print(f\"⚠️ Impossible de lire {img_path}\")\n",
    "            continue\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        scale = min(MAX_POSTER_SIZE / max(w, h), 1.0)\n",
    "        if scale < 1.0:\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            img   = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            w, h  = new_w, new_h\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = orb.detectAndCompute(gray, None)\n",
    "\n",
    "        corners = np.array([\n",
    "            [0, 0],\n",
    "            [w - 1, 0],\n",
    "            [w - 1, h - 1],\n",
    "            [0, h - 1]\n",
    "        ], dtype=np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "        posters.append({\n",
    "            \"name\": img_path.stem,\n",
    "            \"img\": img,\n",
    "            \"kp\": kp,\n",
    "            \"des\": des,\n",
    "            \"corners\": corners,\n",
    "            \"size\": (w, h)\n",
    "        })\n",
    "\n",
    "        print(f\"Affiche chargée : {img_path.stem} ({len(kp)} keypoints)\")\n",
    "\n",
    "    return posters\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DÉTECTION DES POSTERS DANS UNE FRAME\n",
    "# ============================================================\n",
    "\n",
    "def detect_posters_in_frame(frame, posters, orb, bf):\n",
    "    detections = []\n",
    "    if frame is None:\n",
    "        return detections\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = orb.detectAndCompute(gray_frame, None)\n",
    "    if des_frame is None or len(kp_frame) == 0:\n",
    "        return detections\n",
    "\n",
    "    for poster in posters:\n",
    "        des_p = poster[\"des\"]\n",
    "        kp_p  = poster[\"kp\"]\n",
    "        if des_p is None or len(kp_p) == 0:\n",
    "            continue\n",
    "\n",
    "        matches = bf.knnMatch(des_p, des_frame, k=2)\n",
    "        good = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < RATIO_TEST * n.distance:\n",
    "                good.append(m)\n",
    "\n",
    "        if len(good) < MIN_GOOD_MATCHES:\n",
    "            continue\n",
    "\n",
    "        src_pts = np.float32([kp_p[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp_frame[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if H is None:\n",
    "            continue\n",
    "\n",
    "        mask    = mask.ravel().tolist()\n",
    "        inliers = sum(mask)\n",
    "        if inliers < MIN_INLIERS:\n",
    "            continue\n",
    "\n",
    "        corners_trans = cv2.perspectiveTransform(poster[\"corners\"], H)\n",
    "\n",
    "        detections.append({\n",
    "            \"name\": poster[\"name\"],\n",
    "            \"corners\": corners_trans,\n",
    "            \"inliers\": inliers,\n",
    "            \"good_matches\": len(good),\n",
    "            \"H\": H,\n",
    "            \"size\": poster[\"size\"]\n",
    "        })\n",
    "\n",
    "    return suppress_overlaps(detections)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# HEATMAPS\n",
    "# ============================================================\n",
    "\n",
    "def init_heatmaps(posters):\n",
    "    heatmaps = {}\n",
    "    for p in posters:\n",
    "        w, h = p[\"size\"]\n",
    "        hm_w = max(1, w // HEATMAP_DOWNSCALE)\n",
    "        hm_h = max(1, h // HEATMAP_DOWNSCALE)\n",
    "        heatmaps[p[\"name\"]] = np.zeros((hm_h, hm_w), dtype=np.float32)\n",
    "    return heatmaps\n",
    "\n",
    "def update_heatmaps(detections, gaze_x, gaze_y, heatmaps):\n",
    "    for det in detections:\n",
    "        name    = det[\"name\"]\n",
    "        corners = det[\"corners\"].reshape(-1, 2).astype(np.float32)\n",
    "\n",
    "        inside = cv2.pointPolygonTest(corners, (float(gaze_x), float(gaze_y)), False)\n",
    "        if inside < 0:\n",
    "            continue\n",
    "\n",
    "        H_inv = np.linalg.inv(det[\"H\"])\n",
    "        p = np.array([[[gaze_x, gaze_y]]], dtype=np.float32)\n",
    "        uv = cv2.perspectiveTransform(p, H_inv)[0][0]\n",
    "        u, v = uv\n",
    "\n",
    "        w_p, h_p = det[\"size\"]\n",
    "        hm = heatmaps[name]\n",
    "        hm_h, hm_w = hm.shape\n",
    "\n",
    "        ui = int((u / w_p) * hm_w)\n",
    "        vi = int((v / h_p) * hm_h)\n",
    "\n",
    "        if 0 <= ui < hm_w and 0 <= vi < hm_h:\n",
    "            hm[vi, ui] += 1.0\n",
    "\n",
    "def save_heatmaps(heatmaps, posters):\n",
    "    posters_by_name = {p[\"name\"]: p for p in posters}\n",
    "\n",
    "    for name, hm in heatmaps.items():\n",
    "        poster_img = posters_by_name[name][\"img\"]\n",
    "        poster_h, poster_w = poster_img.shape[:2]\n",
    "\n",
    "        if hm.max() > 0:\n",
    "            hm_blur = cv2.GaussianBlur(hm, GAUSSIAN_BLUR_KSIZE, 0)\n",
    "            hm_norm = (hm_blur / hm_blur.max() * 255).astype(np.uint8)\n",
    "        else:\n",
    "            hm_norm = hm.astype(np.uint8)\n",
    "\n",
    "        hm_color = cv2.applyColorMap(hm_norm, cv2.COLORMAP_JET)\n",
    "        hm_resized = cv2.resize(hm_color, (poster_w, poster_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        overlay = cv2.addWeighted(poster_img, 0.6, hm_resized, 0.4, 0)\n",
    "        out_path = f\"Sujet4_heatmap_{name}_V3.png\"\n",
    "        cv2.imwrite(out_path, overlay)\n",
    "        print(f\"✅ Heatmap sauvegardée pour {name} : {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN : PARCOURS VIDÉO + HEATMAPS (PAS DE VIDÉO EN SORTIE)\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    merged = build_merged_gaze_world()\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=ORB_NFEATURES)\n",
    "    bf  = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "\n",
    "    posters = load_posters(orb, POSTERS_DIR)\n",
    "    if not posters:\n",
    "        raise RuntimeError(\"Aucune affiche chargée, vérifie POSTERS_DIR.\")\n",
    "    heatmaps = init_heatmaps(posters)\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Impossible d'ouvrir la vidéo : {VIDEO_PATH}\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    print(\"Début du traitement des frames (heatmaps uniquement)...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame_idx >= len(merged):\n",
    "            break\n",
    "\n",
    "        detections = detect_posters_in_frame(frame, posters, orb, bf)\n",
    "\n",
    "        row = merged.iloc[frame_idx]\n",
    "        x = row[\"gaze x [px]\"]\n",
    "        y = row[\"gaze y [px]\"]\n",
    "\n",
    "        if not (np.isnan(x) or np.isnan(y)):\n",
    "            h, w = frame.shape[:2]\n",
    "            gx = float(np.clip(x, 0, w - 1))\n",
    "            gy = float(np.clip(y, 0, h - 1))\n",
    "            update_heatmaps(detections, gx, gy, heatmaps)\n",
    "\n",
    "        frame_idx += 1\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"{frame_idx} frames traitées...\")\n",
    "\n",
    "    cap.release()\n",
    "    print(\"Fin du parcours vidéo, génération des heatmaps…\")\n",
    "    save_heatmaps(heatmaps, posters)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
